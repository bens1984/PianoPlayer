{\rtf1\ansi\ansicpg1252\cocoartf1038\cocoasubrtf360
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;\red46\green13\blue110;\red63\green110\blue116;\red38\green71\blue75;
\red28\green0\blue207;\red100\green56\blue32;}
\margl1440\margr1440\vieww11120\viewh12060\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\ql\qnatural\pardirnatural

\f0\fs24 \cf0 Specs for Reinforcement Learner\
\
Three components:\
\
0) pre-processing\
1) observer, compressor, predictor\
2) intrinsic reward calculator\
3) RL 'agent' that can take actions to maximize intrinsic reward\
\
0) watch pitch class, interval class, register, change in register, \
\
1) ART to catch categories. Count observations for each category. Report learning mismatch (how much the category would change to fully incorporate it).\
	ART at a higher level. Look at: resonance distances, sequence of categories, pattern of normalized category IDs (ACAF becomes 0102).\
\
2) intrinsic reward maximized with modification (learning) of a category with high resonance (across the whole resonance vector), weighted by observation frequency. The learning of a new category that is close to something we've seen a lot is exciting, learning of new categories that are alone is not.\
	Calculate resonance vector magnitude where each resonance measure is modified by the individual observation count (under a given threshold? 1-1/count? Some sort of curve here to model a sense of "novelty").\
	For each input: get resonance and novelty. compare to each category and get resonance, weighted by occurrence (normalized). Calculate novelty as mismatch with the chosen category (I don't think this needs to be done with all categories, resonance should catch that).\
	When a new category is created it is only as valuable as it is relative to other known categories (if it borders a known category it is very valuable, if it is alone it is not). No residual matches are not valuable (0 reward). Ideal is some residual with highly resonant categories.\
\
2b) extrinsic reward can be injected at any point. This should reinforce the agent's decisions in (3).\
\
3) actions that are taken need to be matched to reward received. Don't want it to just repeat those actions, it needs to understand what it did that was new, what changes it brought about and how to effect those changes again.\
\
\
=======\
Dev Notes:\
9/30\
	added the capability for the ART categories to extend if more input dimensions are added. This allows one ART to watch the resonance vector of another. Thus when the first ART learns a new category the second can tell us if it is distant to what we've seen or close, and parse this into some number of new categories. Now I can watch the learning residual at both levels and compile some sense of learning/novelty/boredom.\
	also watch the unweighted resonance vector, it may contain additional, useful information.\
	Is the decay function of the resonance(category) weighting appropriate? Should it just be a total count, or add a bit and normalize (i.e. steady decay)? Or do we want a total resonance for that category (so even if it's never seen but its neighbors are seen a lot it becomes stronger)\'96I think this would just compound the divergence, and is already taken into account in the resonance*obs_count vector.\
	How can 3) know what produced more reward and do that again, but in new directions?\
	Some amount of random flailing around might work, for a while (just as a baby tries everything). Perhaps this is a last resort to locate new things to learn. What else? Start with a known state and move in a direction, see if it is more interesting?\
	Need some way to correlate outputs (controls into the world) with observed world states. Is this where EM or DBN work comes in? Then the agent could recall outputs given a proposed world state.\
	Try: have each level of the RL network observe control/reward pairs and track them. Then at any point a search can be performed to obtain the best next step. At the base level this will simply be a matter of repeating rewarding controls until they are no longer rewarding (i.e. listen to a melody until it is boring). At the next level it will learn simple transformations (augmentation, transposition, etc.).\
\
10/11\
	I realized that if the space of all inputs is known then the model can test every potential input and predict (know) the reward it would receive if that input comes next. Then remains the problem of bringing that input about (which may be trivial in our toy cases, but hard in real ones). Also, it may run into problems of choosing local maxima (but this may not be a problem either\'85 I'm not sure what the logic is here yet).\
	Should it predict based on the total amount that could be learned from a new input, or how much will actually be learned (as in, how much the ART category will change based on its current Learning Rate parameter?) Is there a difference, really? The most amount that could be learned will translate into the most amount that is learned, even if the learning rate is 10% (or any percent). I set the learning rate low, typically, to encourage some repetition of categories\'85 I will test both ways and see what happens. I still don't think that incorporating the actual learning rate is important here, but I could be wrong.\
\
	It now thinks that creating a new category is awesome, so it tries at every turn (because the residual is huge). How should this be handled? At some point creating a new category is necessary, but should only be chosen if it's more interesting then repeating a category and learning a little bit from it.\
	I hardcoded the residual from a new category, since it is a ridiculous spike otherwise. What should this value be, or is there a smarter way to handle it?\
\
	sometimes a SpatialEncoder is not being assigned in ProcessInput\'85 how does that happen?\
\
	I want it to play with known categories a little longer, then stray away, then work with what it knows, gradually incorporating the new material. Then diverge again, then work it in but maybe in a new direction. At the moment it gets really bored fast and then heads off willy-nilly.\
	Need to add interval classes right away, which will make its choices a little more coherent.\
\
10/12/2011\
\
	Maximize actual learned residual, and use resonance as a tie breaker. In the case of new categories use resonance with known categories as the tie breaker. How should new categories vs. residual be compared?\
\
10/20/2011\
\
	I want it to be able to identify which features are significant and how they are being used. For example: folk music will treat many of the features as fixed (basically) or random, and others are used in specific ways (rhythm is regular but pushed/pulled, pitch is used melodically). How can this be deduced from a series of observations? \
	Perhaps category widths along each dimension, how resonant categories relate together (which dimensions they relate along), between extreme points in the feature space which dimensions are being explored\'85 and how this is happening together (i.e. pitch varies greatly but rhythm remains constant, or timbre is noisy/random while register is very limited).\
\
	Trying to incorporate overall category value (i.e. how many times it shows up or how much it has resonated over the course of the piece) and how fresh or over-played an idea is (a decaying sum of resonance). CategoryValue * (1-overPlayed)? overPlayed is going to be very small values (if normalized) or increasingly large values.\
\
10/21/2011\
	\
	Letting it run for many minutes (10+) produces an interesting observation: it gets slower and slower, as more categories are added and the comparisons get more lengthy. Twice now it suddenly accelerates again, very noticeably. Is this because it has moved to much more "obvious" categorizations and doesn't have to search as deeply? Is there another explanation?\
	It is very bizarre when it happens! It goes from ~130ms per prediction to 70ms or so and then fairly quickly slows down again (over 50 steps?) It could be something that happens coincidentally with the computer (i.e. CPU is freed) but unlikely, this computer has 8 powerful cores (and I'm maxing 1 fully).\
\
10/24/2011\
\
	These settings work pretty nicely:\
		art1 - 0.1 LR, 0.95 vig, 0.8 decays, 0.4 decay on "others"\
		upperArt - 0.05 LR, 0.6 Vig\
		0.5 Import Factor, 0.975 recency decay, useRecency=false\
\
10/25/2011\
\
	In 2nd tier ART try using just the category ID in a standard STM, and also try using the fit vector as stimulus for an STM (so the resonance excites if it's > than the current level, otherwise it just decays). Which gives a better sense of the progression without losing too much information (the cat ID might be too lossy)?\
	Perhaps add an ART that encodes the transitions between first level STM steps? (absolute value of each element - previous time step of same element) This might make some very interesting patterns as the RL tries to maximize this.\
	The 2nd tier ART (thirdART) should have a dynamic vigilance setting. As more dimensions are added to the feature vector it will become more particular, but it means that at first it is very accepting. I'm assuming this is not good, but maybe it's a feature?\
\
10/26/2011\
\
	Trying to understand the effects of LR and vigilance on the first and 3rd level arts (there is no 2nd level at the moment). Reward function is:\
\pard\tx560\pardeftab560\ql\qnatural\pardirnatural

\f1\fs22 \cf2 \CocoaLigature0 sin\cf0 (\cf2 pow\cf0 (\cf3 myArt\cf0 ->\cf4 GetResidual\cf0 (), \cf5 0.5\cf0 ) * \cf5 3.0\cf0 ) * \cf2 pow\cf0 (importSum, \cf6 IMPORTANCE_FACTOR\cf0 ) + \cf2 sin\cf0 (\cf2 pow\cf0 (\cf3 thirdArt\cf0 ->\cf4 GetResidual\cf0 (), \cf5 0.125\cf0 ) * \cf6 M_PI\cf0 ) * \cf5 4.0\cf0 ;
\f0\fs24 \CocoaLigature1 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\ql\qnatural\pardirnatural
\cf0 	With 1st at 0.5/0.925 and 3rd at 0.5/0.98 it forces a high degree of similarity, and very slow progression through space. 3rd: 0.5/0.02 is so similar, I can't stipulate the difference.\
	Setting 1st to 0.1/0.925 since this has interesting results when 3rd is turned off.\
	Using seed: 60, 62, 64, 65, 67 and then it asks for 69 to run.\
	Comparing 3rd 0.1/0.2 and 0.1/0.8 I can't categorize (or identify) the difference! 0.8 Is highly minimalist, but progresses to other ideas. 0.85 is much more fragmented, but still highly repetitious, basically covering the same ground but in a different order. 0.9 is more like 0.8, it seems that 0.85 (and perhaps different "pockets") are very different! I can't find any others yet\'85 0.845 diverges later but is more like 0.8 at first. 0.855 is good, 0.86 is back again. At 0.425 it repeats the same pattern over and over and over.\
	0.01/0.85 goes back to the same old, same old. In fact' it's far more repetitious (as I might expect. 0.01 LR makes the categories form very slowly). 0.9/0.85 has some moments of long repetition (at the 12th 3rd category), but gradually moves further and further afield. 0.6/0.85 seems ok, in the middle I guess (it moves down an octave at category 27, which is unique).\
	Found an error in the GetImportanceSum and fixed reward function to:\
\pard\tx560\pardeftab560\ql\qnatural\pardirnatural

\f1\fs22 \cf2 \CocoaLigature0 sin\cf0 (\cf2 pow\cf0 (\cf3 myArt\cf0 ->\cf4 GetResidual\cf0 (), \cf5 0.5\cf0 ) * \cf5 3.0\cf0 ) * \cf2 pow\cf0 (importSum, \cf6 IMPORTANCE_FACTOR\cf0 ) + \cf2 sin\cf0 (\cf2 pow\cf0 (\cf3 thirdArt\cf0 ->\cf4 GetResidual\cf0 (), \cf5 0.125\cf0 ) * \cf5 3.1\cf0 ) * \cf5 4.0\cf0  * thirdImportSum;\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\ql\qnatural\pardirnatural

\f0\fs24 \cf0 \CocoaLigature1 	I need to incorporate distance between feature vectors at the next level. I think that's the place. Also, watching the variance of each feature dimension over long windows should give an idea of constraints, or style.\
\
10/27/2011\
\
	How do I split up STMs/ARTs? Should I take the Gjerdingen approach and lump it all into one big feature vector? My initial thoughts are to split out anything that doesn't always need to correlate. For example, the distance and curvature between feature vectors doesn't need to be explicitly tied to specific pitch sets. However, interval pattern is much more closely tied to pitch sequence. However, these could both be split apart and then compared at a second order ART.\
	Perhaps the big lump approach is appropriate for classical music, where the elements (pitch, interval, rhythm) are not inseparable, but highly linked (chords change on the beat/bar, chord tones/passing tones tend to fall in different places in the meter). As opposed to more contemporary work where they are considered to be very separable (color/talia, etc.).\
	Still, the same effect could be achieved with several STM/ARTs (one for each feature group) and then the linkage between them could be enforced more or less strictly (more reward for same/difference at the 2nd level).\
\
What it's doing:\
	At each level (where an ART is in place and a resultant reward calculation) the RL is trying to incrementally explore the feature space. Due to the nature of spatial encoding this requires a notion of repetition & variation. If the ART is given another feature, it will try to vary it over time and find different relationships between the features.\
	The nesting of ARTs causes \'85\
\
10/31/2011\
\
	Trying to give the RL more sense of direction.\
	Perhaps the multiple ARTs with dif vigilance widths at the same level is useful?\
	What about splitting pitch-class/tonality and interval (and others)? Then correlating their results?\
		This could allow looking at PC order and PC occurrence (i.e. tonality), and doing the same for interval.\
\
11/1/2011\
\
From handwritten notes:\
	how to give RL baby the tools to discover meter as an organizing scheme for music?\
	Reconcile rewards from different areas: rhythm, pitch, ? simultaneously?\
	Deal w/ frequency of appearance of categories. Atm areas w/ more categories have more resonance regardless of interest (straight resonance, i.e. sum, is not right/good?). Results in strong inertia. Maybe idea of recency? Very recent + fresh = good, very old is good too, overplayed is bad.\
	Fresh = high residual\
	Recall = high import\
	Overplayed = boring = high occurrence + no residual\
	Track "play value" + recency? For a given "piece" to a career.\
	Model is stream of consciousness, not crafted composition. But improvisation could be identified as a RL operation. Typically informed by years of practice, but the kernel should be there from the start (the tools, features to identify, etc.).\
	Interruptions or Leaps: work in an area continuously/gradually, until some threshold & then make a "jump" (to a slightly less related area).\
	What is this?\
\
	Simply look at residual w/ best match, ignoring possibility of a new category? (i.e. no vigilance check) Look for "some" residual? Then it may actually make a new category. Tie to recency/import/resonance? Residual alone maximizes distance, Resonance maximizes cohesion.\
----\
	Any given feature will be explored in a continuous fashion, aiming for all possibilities. Derived features are necessary here to guide that exploration and reward internal cohesion with variation, rather than "random" (to us) wandering. For example, simply giving the RL 12 pitch classes will result in all tone rows and sub-rows eventually, but without any progression. Interval Classes aids this, encouraging exploration of interval relationships too.\
	How can we model changes between categories? I can watch the resonance vectors and categorize those, but that doesn't give transitions. I can make an STM of the category IDs, but that loses closely related categories and is thus very finicky. Is the distance between successive resonance vectors significant? Perhaps feed that category ID into an STM and learn that!\
		first order categorization of everything -> measure distance between resonance vectors -> categorize this -> feed this ID into a spatial encoder -> categorize this STM -> and watch for reward here!\
	In code: bigArt for everything -> distances in vectors -> upperArt watches distance, learns -> category ID is fed into thirdSTM\
\
11/2/2011\
\
	We have these complex feature vectors now, perhaps too rich. For example: most recent PC and the interval vector serves to recover the previous material. Characterizing repetitions in that sequence. Better tonality representation (keep vector at 1s, then *0.5 for every occurrence of a pitch? Decay by adding 0.15 or so)\
\
11/3/2011\
	settings at 0.6 LR, 0.925 Vig on most, 0.4 LR on others, 0.5, 0.8 on big, 0.5, 0.925 on third & derived\
\
11/4/2011\
	set the "distance" watcher to watch the amount of movement in the resonance vector of the "bigART"(watches all of the first level (L1) feature vectors combined). It now has a very interesting curve while following the Bach Cello suite1 prelude. I have it watching the distance between input at t and t-1, t and an average of 2, average of 4, and average of 8 (low pass filter). This should catch rapid alternations vs steady movement in a direction. Curvature should help this too.\
\
11/8/2011\
\
	rewinding the progress of reward functions to try and understand it piece by piece.\
	First: going for a flat rate reward for learning residual. If it's measurable then we call it good, otherwise it's boring. Then we measure chaotic inputs by how much resonance the input generates, setting some sort of threshold for this measure. Things that are deemed really chaotic shouldn't be learned at all, or marked as a category that is unintelligible to the agent (i.e. "I heard this word but I don't know what it means and I can barely remember it").\
	I could block the vigilance check when predicting (calculating reward for a given input). Distant inputs then train new categories which will always have a residual equal to the number of dimensions (or be locked to some constant). Their resonance will be good still.\
\
	BECAUSE import is currently calculated as the sum of all the resonances it will always try to build uniformly around what it knows. But maybe we want it to be willing to wander in a direction, mapping a more uneven shape in the feature space.\
	TRYING: just looking at the residual, rewarding if it is in a specific band, otherwise it is chaotic or boring. Block vigilance check on prediction, if something is close and trains a new category that is good, if we allow vigilance checks it will create a new category which has a high residual by default, and would always be unrewarding (I will try with vigilance checks on as a second step, to see if it still creates categories accidentally, or as a result of another ART voting high).\
	Keep vig on, just look at residual and resonance with chosen match (ignore the rest of the field). This will allow it to wander off in a direction. Still need to ensure that repetitions are encouraged, that creating new categories is relatively discouraged (but allowed), \
\
11/13/2011\
\
	Calculating Import based on the best match, without a vigilance check, is promising thus far. It still has patterns I can hear (and like), but is not as thorough and mundane in its movement. I'm using a switched residual: either it's over a given a threshold and thus potentially interesting, or else it is boring. Residual * Importance is the reward (adding the different ART outputs together).\
	Now I'm thinking about how to give it an idea of phrasing, of rhythm (which should aid phrasing), and of greater directionality (direction over several notes - maybe an average?). Do we hear phrases as ascending/descending in a binary fashion, or does the degree of ascending/descending matter much? Surely there must be cases where the degree of ascension is important.\
	How can a phrase shape (continuous over a number of notes) be encoded into an ART readable/comparable form?\
\
11/14/2011\
\
	Intrinsic Reward: the joy of playing, of experimenting, of being creative.\
	Extrinsic Reward: the joy of reproducing something that matches externally perceived patterns (i.e. stylistic constraints).\
\
	I need to improve intrinsic reward a little, and give it features to handle phrase shape, directionality, rhythm (maybe dynamics?). Then add extrinsic reward, at first in the form of "externally identified categories always give X extrinsic reward." This can be done by tagging input as internally produced vs. externally produced (from a Bach sonata or someone improvising). Then the ARTs will be encouraged to play those categories and improvise from them (care must be taken to balance the ext/int rewards so that it doesn't get stuck on a few externally rewarding categories and become a broken record).\
\
	Idea for the first paper: Composition Lesson with a Reinforcement Learning Agent. Go through a series of different designs (intrinsic reward), analyzing its output and discussing the reasons. Give a nice introduction and justification: RL presents a compelling argument for describing aesthetic pleasure, creativity, beauty, etc. Such a biologically inspired model may be able to produce interesting, convincing music. Then compare a version with a short feature vector, one with a full feature vector, and different reward structures (considering cumulative importance, importance & residual, and residual alone).\
\
	I can process rhythm as inter-onset-interval, measured at the end of the interval (i.e. seen in combination with the pitch of the new event). How will prediction work, then? It will have to look out one step, in order to give duration. Or the synthesis will just be a little stilted, using the duration given as a pause (duration for the previous event) and then playing the new note.\
	How many ratios will be necessary? 1:1, :2, :3, :4, :5, :6, :7, 2:3, :5, :7, 3:4, 3:5, 3:7, 4:5, 4:7, 5:6, 5:7, 6:7}